{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to determine how much outlier data we want to exclude from our dataset, as doing so will reduce the number of training features available. I will decide to only remove the outliers for the top three most correlated features. The top three most correlated features, V14, V4, and V12, all have a significant number of non-fraud outliers that intersect with the quartiles of the fraud box plots. While the correlation coefficients for the next two most correlated features (V11 and V10) are also quite high, they do not have as many outliers that intersect with the opposite box plots. The V16 feature class has some outliers that intersect, but given its lower correlation coefficient, I will choose to only remove outliers from the top three most correlated features to minimize the necessary outlier removals.\n",
    "\n",
    "We will use the interquartile range method to remove outlier data from our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom scorer, we only care about getting as many true negatives as possible\n",
    "precision_true_neg = make_scorer(precision_score, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision trees\n",
    "\n",
    "dt_hyperparams = {\n",
    "        \"max_depth\": [None, 2, 8],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_leaf_nodes\": [None, 5, 10]\n",
    "    }\n",
    "\n",
    "decision_trees = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(\n",
    "        estimator=decision_trees, \n",
    "        param_grid=dt_hyperparams, \n",
    "        scoring=precision_true_neg,\n",
    "        cv=5, \n",
    "        verbose=2,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "dt_model = clf.fit(xtrain, ytrain)\n",
    "print(\"Best parameters found from grid search: \", clf.best_params_)\n",
    "print(\"Validation fitting with best precision on labels = 1:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_k = 2\n",
    "max_k = 20\n",
    "n_array = np.arange(min_k, max_k, 3)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_hyperparams = {\n",
    "    'n_neighbors': n_array,\n",
    "    'weights': ['uniform', 'distance']\n",
    "    }\n",
    "\n",
    "clf = GridSearchCV(estimator=knn, \n",
    "                          param_grid=knn_hyperparams,\n",
    "                          scoring= precision_true_neg, \n",
    "                          cv=5,  \n",
    "                          verbose=2, \n",
    "                          n_jobs=-1)\n",
    "\n",
    "knn_model = clf.fit(xtrain, ytrain)\n",
    "print(\"Best parameters found from grid search: \", clf.best_params_)\n",
    "print(\"Validation fitting with best precision on labels = 1:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "\n",
    "logistic_hyperparams = {\n",
    "    \"penalty\": ['l1', 'l2'], \n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "\n",
    "clf = GridSearchCV(estimator=logistic, \n",
    "                          param_grid=logistic_hyperparams,\n",
    "                          scoring= precision_true_neg, \n",
    "                          cv=5,  \n",
    "                          verbose=2, \n",
    "                          n_jobs=-1)\n",
    "\n",
    "logistic_model = clf.fit(xtrain, ytrain)\n",
    "\n",
    "print(\"Best parameters found from grid search: \", clf.best_params_)\n",
    "print(\"Validation fitting with best precision on labels = 1:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "\n",
    "svm_hyperparams = {\n",
    "    'C': [0.5, 0.7, 0.9, 1], \n",
    "    'kernel': ['rbf', 'poly', 'sigmoid', 'linear']\n",
    "    }\n",
    "\n",
    "clf = GridSearchCV(estimator=svm, \n",
    "                          param_grid=svm_hyperparams,\n",
    "                          scoring= precision_true_neg, \n",
    "                          cv=5,  \n",
    "                          verbose=2, \n",
    "                          n_jobs=-1)\n",
    "\n",
    "svm_model = clf.fit(xtrain, ytrain)\n",
    "\n",
    "print(\"Best parameters found from grid search: \", clf.best_params_)\n",
    "print(\"Validation fitting with best precision on labels = 1:\", clf.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc371",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
